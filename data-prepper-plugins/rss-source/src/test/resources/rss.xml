<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:discourse="http://www.discourse.org/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>OpenSearch - Latest topics</title>
        <link>https://forum.opensearch.org/latest</link>
        <description>Latest topics</description>
        <lastBuildDate>Sun, 08 Oct 2023 03:50:26 +0000</lastBuildDate>
        <atom:link href="https://forum.opensearch.org/latest.rss" rel="self" type="application/rss+xml" />
        <item>
            <title>Update defaulRoute, timepicker:timeDefaults via curl in opensearch-dashboards</title>
            <dc:creator>
                <![CDATA[Manish]]>
            </dc:creator>
            <category>OpenSearch Dashboards</category>
            <description>
                <![CDATA[
            <p><strong>.8.0</strong> (relevant - OpenSearch/Dashboard/Server RockyLinux 8/Chromer):</p><p><strong>Describe the issue</strong>:</p><p>I am new to opensearch.<br>
I am not able to upate the defaulRoute, timepicker:timeDefaults etc settings which is under AdvanceSettings in opensearch-dashboard via curl command.</p><p><strong>Configuration</strong>:<br>
I tried below commands, i got some output but the page is not redirected to /app/discover as weel the settings in the UI still show /app/home</p><h2><a name="command-1" class="anchor" href="https://forum.opensearch.org#command-1"></a>Command</h2><p>curl -u ‘****:*****’ -k -X POST “<a href="http://localhost:5601/api/saved_objects/config" rel="noopener nofollow ugc">http://localhost:5601/api/saved_objects/config</a>” -H “osd-xsrf: true” -H “content-type: application/json; charset=utf-8” -H “securitytenant: global” -d ‘{“attributes”: {“defaultRoute”: “/app/discover”}}’</p><h2><a name="output-2" class="anchor" href="https://forum.opensearch.org#output-2"></a>OUTPUT</h2><p>{“type”:“config”,“id”:“81288fd0-658c-11ee-b242-b3c553df404d”,“attributes”:<br>
{“defaultRoute”:“/app/discover”},“references”:<span class="chcklst-box fa fa-square-o fa-fw"></span>,“migrationVersion”{“config”:“7.9.0”},“updated_at”:“2023-10-08T03:41:03.181Z”,“version”:“WzE2NCw0XQ==”,“namespaces”:[“default”]}</p><p>I searched on internet and found another command but it gave me error<br>
curl -u ‘<em><strong>:</strong></em>’ -k -X POST “<a href="http://localhost:5601/api/opensearch-dashboards/settings" rel="noopener nofollow ugc">http://localhost:5601/api/opensearch-dashboards/settings</a>” -H “osd-xsrf: true” -H “content-type: application/json; charset=utf-8” -H “securitytenant: global” -d ’<br>
{ “attributes”: {<br>
“defaultRoute”:“/app/discover”<br>
}<br>
}’</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/update-defaulroute-timepicker-timedefaults-via-curl-in-opensearch-dashboards/16184">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/update-defaulroute-timepicker-timedefaults-via-curl-in-opensearch-dashboards/16184</link>
            <pubDate>Sun, 08 Oct 2023 03:50:26 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16184</guid>
            <source url="https://forum.opensearch.org/t/update-defaulroute-timepicker-timedefaults-via-curl-in-opensearch-dashboards/16184.rss">Update defaulRoute, timepicker:timeDefaults via curl in opensearch-dashboards</source>
        </item>
        <item>
            <title>Dual Root CA Configuration: Distinguishing Transport-Layer and REST-Layer Communications</title>
            <dc:creator>
                <![CDATA[hm21]]>
            </dc:creator>
            <category>Security</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Version 2.10<br>
Ubuntu 22.04</p><p><strong>Describe the issue</strong>:<br>
Is it feasible to utilize distinct root CA certificates for transport-layer and REST-layer communications? Specifically, I’d like to implement a self-signed root CA certificate for transport-layer (node-to-node) communication, while employing a trusted Certificate Authority for the REST-layer (client-to-node) traffic.</p><p>Can this configuration be achieved? Additionally, is it possible to utilize the securityadmin.sh script to update the security settings accordingly?</p><p><strong>Configuration</strong>:<br>
Docker-Compose</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/dual-root-ca-configuration-distinguishing-transport-layer-and-rest-layer-communications/16180">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/dual-root-ca-configuration-distinguishing-transport-layer-and-rest-layer-communications/16180</link>
            <pubDate>Sat, 07 Oct 2023 17:12:04 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16180</guid>
            <source url="https://forum.opensearch.org/t/dual-root-ca-configuration-distinguishing-transport-layer-and-rest-layer-communications/16180.rss">Dual Root CA Configuration: Distinguishing Transport-Layer and REST-Layer Communications</source>
        </item>
        <item>
            <title>Data Prepper can not write to Opensearch Datastream [BUG] #2037</title>
            <dc:creator>
                <![CDATA[godin.igor]]>
            </dc:creator>
            <category>Data Prepper</category>
            <description>
                <![CDATA[
            <p>Hello.<br>
There is a <a href="https://github.com/opensearch-project/data-prepper/issues/2037" rel="noopener nofollow ugc">bug report</a> on github? And we get the same issue. Can not send logs to Opensearch Data stream:</p><p>Error “WARN com.amazon.dataprepper.plugins.sink.opensearch.OpenSearchSink - Document [org.opensearch.client.opensearch.core.bulk.BulkOperation@c4aecd] has failure: java.lang.RuntimeException: only write ops with an op_type of create are allowed in data streams”</p><p><strong>Configuration</strong>:<br>
data-prepper:latests, opensearch v 2.10.0,</p><pre><code class="lang-auto">kafka-sysmon_security_eventlog-pipeline:
  source:
    kafka:
      acknowledgments: true
      encryption:
          type: none
      bootstrap_servers:
        - xxxxxxxx:9094
        - xxxxxxxx:9095
        - xxxxxxxx:9096
      topics:
        - name: "sysmon_security_eventlog"
          group_id: "data_prepper"
          key_mode: "discard"
          serde_format: "json"
          auto_commit: true
  processor:
    - aggregate:
          identification_keys: ["event.provider","event.code","event.outcome","host.name","winlog.event_data.AuthenticationPackageName","winlog.event_data.TargetDomainName","winlog.event_data.TargetUserName","winlog.event_data.TargetUserSid","winlog.event_data.WorkstationName"]
          action:
            remove_duplicates:
          group_duration: 30s
  sink:
    - opensearch:
          hosts: ["https://xxxxxxxx:9200"]
          username: xxxxxxxx
          password: xxxxxxxx
          insecure: true
          connect_timeout: 60000
          index: logs-events-sysmon_security_eventlog
          index_type: management_disabled
</code></pre><p>Is any progress there ?<br>
The bug has been open for almost a year and so far nothing has moved, whereas this feature would be very necessary.<br>
Or is there some workaround?</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/data-prepper-can-not-write-to-opensearch-datastream-bug-2037/16175">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/data-prepper-can-not-write-to-opensearch-datastream-bug-2037/16175</link>
            <pubDate>Fri, 06 Oct 2023 13:58:06 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16175</guid>
            <source url="https://forum.opensearch.org/t/data-prepper-can-not-write-to-opensearch-datastream-bug-2037/16175.rss">Data Prepper can not write to Opensearch Datastream [BUG] #2037</source>
        </item>
        <item>
            <title>OpenSearch only receives logs once per hour</title>
            <dc:creator>
                <![CDATA[Ziggiyzoo]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Logstash - 7.17.11<br>
Logstash OpenSearch Output - 1.2.0<br>
OpenSearch - Helm Chart Version 2.9.0</p><p><strong>Describe the issue</strong>:<br>
Currently we have Elasticsearch set up, but we are moving to OpenSearch. I’ve set up OpenSearch output following the guide, but OpenSearch is only receiving logs once per hour, while Elasticsearch receives them almost constantly.</p><p>The Logstash logs don’t seem out of the ordinary, so I am not sure if this is an issue with the OpenSearch Output setup, or some OpenSearch config I missed.</p><p><strong>Configuration</strong>:<br>
Logstash Output:</p><pre><code class="lang-auto">output {
  if "event-timed-out" in [tags] {
    elasticsearch {
      id =&gt; "fallback-elasticsearch-output"
      hosts =&gt; ["localhost:9200"]
      index =&gt; "fallback-bisappslogs-%{+YYYY.MM.dd}"
    }
    opensearch {
        index =&gt; "fallback-bisappslogs-%{+YYYY.MM.dd}"
        hosts =&gt; ["host"]
        user  =&gt; "${}"
        password =&gt; "${}"
        ssl_certificate_verification =&gt; true
    }
  } else {
    elasticsearch {
      id =&gt; "elasticsearch-output"
      hosts =&gt; ["localhost:9200"]
      index =&gt; "bisappslogs-%{+YYYY.MM.dd}"
    }
    opensearch {
        index =&gt; "bisappslogs-%{+YYYY.MM.dd}"
        hosts =&gt; ["host"]
        user  =&gt; "${}"
        password =&gt; "${}"
        ssl_certificate_verification =&gt; true
    }
  }
}
</code></pre><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/opensearch-only-receives-logs-once-per-hour/16173">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/opensearch-only-receives-logs-once-per-hour/16173</link>
            <pubDate>Fri, 06 Oct 2023 09:55:39 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16173</guid>
            <source url="https://forum.opensearch.org/t/opensearch-only-receives-logs-once-per-hour/16173.rss">OpenSearch only receives logs once per hour</source>
        </item>
        <item>
            <title>Logstash Opensearch Input</title>
            <dc:creator>
                <![CDATA[jim45515]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Latest.  Deployed in docker containers.</p><p><strong>Describe the issue</strong>:<br>
I’m trying to create a new Logstash pipeline which uses the OpenSearch input plugin.  Seems a fairly simple setup, but some reason I’m getting the following error:</p><p>PKIX path building failed, unable to find valid certificate path to requested target</p><p>I have other OpenSearch outputs that are working, so bit confused why this is broken?</p><p>Any idea of what to check appreciated.</p><p><small>7 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/logstash-opensearch-input/16168">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/logstash-opensearch-input/16168</link>
            <pubDate>Thu, 05 Oct 2023 17:45:27 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16168</guid>
            <source url="https://forum.opensearch.org/t/logstash-opensearch-input/16168.rss">Logstash Opensearch Input</source>
        </item>
        <item>
            <title>Why my OpenSearch vector search is slow</title>
            <dc:creator>
                <![CDATA[Garance]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
OpenSearch 1.3 on AWS</p><p><strong>Describe the issue</strong>:<br>
I have a OpenSearch Service instance on AWS with 2 nodes, I created an index with a knn field. My queries took 10 seconds.<br>
The index contains more than 2 millions of data with size of 11gb.</p><p>How to improve the knn search?</p><p><strong>Configuration</strong>:<br>
Schema:</p><pre><code class="lang-auto">{
  "settings": {
    "index": {
      "number_of_shards": 2,
      "number_of_replicas": 0,
      "knn": true,
      "knn.algo_param.ef_search": 512
    }
  },
  "mappings": {
    "dynamic": "false",
    "properties": {
      "title": {
        "type": "text"
      },
      "text": {
        "type": "text"
      }
      "vector": {
        "type": "knn_vector",
        "dimension": 256,
        "method": {
          "name": "hnsw",
          "space_type": "l2",
          "engine": "nmslib",
          "parameters": {
            "ef_construction": 2000,
            "m": 16
          }
        }
      }
    }
  }
}
</code></pre><p>Query:</p><pre><code class="lang-auto">{
  "highlight": {
    "fields": {
      "text": {}
    },
    "fragment_size": 1000,
    "fragmenter": "simple",
    "highlight_query": {
      "match": {
        "text": "Why my OpenSearch vector search is slow?"
      }
    },
    "no_match_size": 1000,
    "number_of_fragments": 5,
    "post_tags": "&lt;/em&gt;",
    "pre_tags": "&lt;em&gt;"
  },
  "_source": [
    "title",
    "text"
  ],
  "size": 100,
  "query": {
    "knn": {
      "vector": {
        "k": 100,
        "vector": [0.06414795,-0.033294678,...,-0.06677246]
      }
    }
  }
}
</code></pre><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/why-my-opensearch-vector-search-is-slow/16166">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/why-my-opensearch-vector-search-is-slow/16166</link>
            <pubDate>Thu, 05 Oct 2023 17:10:42 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16166</guid>
            <source url="https://forum.opensearch.org/t/why-my-opensearch-vector-search-is-slow/16166.rss">Why my OpenSearch vector search is slow</source>
        </item>
        <item>
            <title>AWS OpenSearch service lost one node during indexation of knn vectors</title>
            <dc:creator>
                <![CDATA[Garance]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
OpenSearch 1.3 on AWS</p><p><strong>Describe the issue</strong>:<br>
I have a OpenSearch Service instance on AWS with 2 nodes, during each indexation with knn vectors, one node was lost near the end of indexation. Thanks to the automatic remediation of red clusters, the lost node was restored 30 minutes later.</p><p>The vector index contains some text fields and a knn field, with more than 2 millions documents, the total size is 11gb in index.<br>
I have a Python3 program to get data from an index with scroll, and run vector indexation with _bulk</p><p>How to avoid the problem of lost node?</p><p><strong>Configuration</strong>:<br>
OpenSearch Service instance: type c6g.xlarge.search on AWS, 2 nodes with 4 vCPU, 8 Gb RAM, 200 Gb storage, 6000 IOPS, 256 Mo/s of each node</p><p>Schema:</p><pre><code class="lang-auto">{
  "settings": {
    "index": {
      "number_of_shards": 2,
      "number_of_replicas": 0,
      "knn": true,
      "knn.algo_param.ef_search": 512
    }
  },
  "mappings": {
    "dynamic": "false",
    "properties": {
      "title": {
        "type": "text"
      },
      "text": {
        "type": "text"
      }
      "vector": {
        "type": "knn_vector",
        "dimension": 256,
        "method": {
          "name": "hnsw",
          "space_type": "l2",
          "engine": "nmslib",
          "parameters": {
            "ef_construction": 2000,
            "m": 16
          }
        }
      }
    }
  }
}
</code></pre><p><strong>Relevant Logs or Screenshots</strong>:<br>
IndexingRate<br></p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/7/7b05427c19a324379fddcc79a07a3daf8fdf5805.png" data-download-href="/uploads/short-url/hyhZRhRcCY0IzvYLKzpOEdeFfo1.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/7/7b05427c19a324379fddcc79a07a3daf8fdf5805_2_690x256.png" alt="image" data-base62-sha1="hyhZRhRcCY0IzvYLKzpOEdeFfo1" width="690" height="256" srcset="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/7/7b05427c19a324379fddcc79a07a3daf8fdf5805_2_690x256.png, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/7/7b05427c19a324379fddcc79a07a3daf8fdf5805_2_1035x384.png 1.5x, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/7/7b05427c19a324379fddcc79a07a3daf8fdf5805.png 2x" data-dominant-color="FDFEFE"></a></div><p></p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/aws-opensearch-service-lost-one-node-during-indexation-of-knn-vectors/16165">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/aws-opensearch-service-lost-one-node-during-indexation-of-knn-vectors/16165</link>
            <pubDate>Thu, 05 Oct 2023 16:42:20 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16165</guid>
            <source url="https://forum.opensearch.org/t/aws-opensearch-service-lost-one-node-during-indexation-of-knn-vectors/16165.rss">AWS OpenSearch service lost one node during indexation of knn vectors</source>
        </item>
        <item>
            <title>Filebeat 7.17 not connecting to OpenSearch</title>
            <dc:creator>
                <![CDATA[McJava1967]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p>Hi all.  We’re trying to switch from Elastic Stack to OpenSearch.  OpenSearch and Dashboards are now running.  However, our old Filebeat 7.17 won’t connect.  I know they’re intentionally blocking OpenSearch after 7.13.  I tried “override_main_response_version”: true, but no luck.</p><p>So, my questions:</p><p>Is it hopeless to continue with Filebeat?</p><p>When OpenSearch forked, did they also fork a version of FileBeat?</p><p>If we have to give up on Filebeat, could anyone point me to other options?  Any recommendations?  Our Filebeat needs are fairly simple.  We want something light-weight that we can get up and running quickly.</p><p>THANK YOU!!  <img src="https://emoji.discourse-cdn.com/google/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p><p><small>5 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/filebeat-7-17-not-connecting-to-opensearch/16163">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/filebeat-7-17-not-connecting-to-opensearch/16163</link>
            <pubDate>Thu, 05 Oct 2023 14:36:13 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16163</guid>
            <source url="https://forum.opensearch.org/t/filebeat-7-17-not-connecting-to-opensearch/16163.rss">Filebeat 7.17 not connecting to OpenSearch</source>
        </item>
        <item>
            <title>Java Client - Class Specification</title>
            <dc:creator>
                <![CDATA[docme]]>
            </dc:creator>
            <category>OpenSearch Client Libraries</category>
            <description>
                <![CDATA[
            <p>The new Java Client requires a Java Class to specify the fields to be returned from a search request. I have reviewed the github pull request for Adding new generic client, <span class="hashtag-raw">#417</span> (<a href="https://github.com/opensearch-project/opensearch-java/pull/415" class="inline-onebox" rel="noopener nofollow ugc">Adding new generic client by harshavamsi · Pull Request #415 · opensearch-project/opensearch-java · GitHub</a>) and<span class="hashtag-raw">#377</span> (<a href="https://github.com/opensearch-project/opensearch-java/issues/377" class="inline-onebox" rel="noopener nofollow ugc">[FEATURE] Enable Generic HTTP Actions in Java Client · Issue #377 · opensearch-project/opensearch-java · GitHub</a>). Its not clear to me if this client will ultimately support specifying the return fields as currently provided by HLRC.</p><p>{ “size”: 1000, “query”: { “bool”: { “filter”: [ { “term”: { “module”: { “value”: “ncs”, “boost”: 1 } } } ], “adjust_pure_negative”: true, “boost”: 1 } }, “_source”: { “includes”: [ “module”, “descp”, “_id” ], “excludes”: <span class="chcklst-box fa fa-square-o fa-fw"></span> }, “sort”: [ { “module”: { “order”: “asc” } } ] }</p><p>I would like to know if the new generic client will support json requests that specify _source “includes” or “excludes” as an option to defining the return fields via a Java class, and if a generic client is planned before HLRC support is removed in the 4.0 release. This approach would remove the need to filter out unwanted class fields and reduce bandwidth costs over time as well.</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/java-client-class-specification/16161">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/java-client-class-specification/16161</link>
            <pubDate>Thu, 05 Oct 2023 13:43:23 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16161</guid>
            <source url="https://forum.opensearch.org/t/java-client-class-specification/16161.rss">Java Client - Class Specification</source>
        </item>
        <item>
            <title>Failing to send traces from Opentelemetry DataPrepper to Opensearch</title>
            <dc:creator>
                <![CDATA[infemus]]>
            </dc:creator>
            <category>Data Prepper</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> OpenSearch 2.10/Dashboard 2.10/Server OS MacOS/Browser Docker</p><p><strong>Describe the issue</strong>:<br>
I setup a docker-compose file to try to send Node.js traces to Opensearch using Opentelemetry and Data prepper but I keep getting error messages from opentelemetry that it is not able to connect to data prepper.<br>
Data prepper is also displaying an error about not being able to create the opensearch sink connector Caused by: java.io.IOException: opensearch: Name or service not known</p><p>I tried changing the name of the service in the data-prepper config but that did not work.</p><p><strong>Configuration</strong>:<br>
Here are the docker-compose files for Opentelemetry, Data Prepper, Opensearch and Opensearch-Dashboards</p><pre><code class="lang-auto">version: "3"

services:
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.59.0
    command:
      - "--config"
      - "/otel-local-config.yaml"
    volumes:
      - ./tracetest/collector.config.yaml:/otel-local-config.yaml
    depends_on:
      data-prepper:
        condition: service_started

  data-prepper:
    restart: unless-stopped
    image: opensearchproject/data-prepper:1.5.1
    volumes:
      - ./tracetest/opensearch/opensearch-analytics.yaml:/usr/share/data-prepper/pipelines.yaml
      - ./tracetest/opensearch/opensearch-data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml
    depends_on:
      opensearch:
        condition: service_started

  opensearch:
    # container_name: node-3.example.com
    image: opensearchproject/opensearch:2.10.0
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9600:9600"
    # healthcheck:
    #   test: curl -s http://localhost:9200 &gt;/dev/null || exit 1
    #   interval: 5s
    #   timeout: 10s
    #   retries: 5
    networks:
      - my_network

  dashboards:
    image: opensearchproject/opensearch-dashboards:2.10.0
    container_name: opensearch-dashboards3
    ports:
      - "5601:5601"
    expose:
      - "5601"
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
    depends_on:
      - opensearch
    networks:
      - my_network

networks:
  my_network:
</code></pre><p>The node.js app is in a separate docker-compose.yml file below:</p><pre><code class="lang-auto">version: '3'
services:
  app:
    image: quick-start-nodejs
    build: .
    ports:
      - "8080:8080"
    networks:
      - my_network

networks:
  my_network:

</code></pre><p>this is the  Dockerfile also for the node.js app:</p><pre><code class="lang-auto">FROM node:slim
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 8080
CMD [ "npm", "run", "with-grpc-tracer" ]

</code></pre><p>I used this document [Running Tracetest with OpenSearch | Tracetest Docs](<a href="https://docs.tracetest.io/examples-tutorials/recipes/running-tracetest-with-opensearch" class="inline-onebox" rel="noopener nofollow ugc">Running Tracetest with OpenSearch | Tracetest Docs</a> to get things up and running but then the configuration did not have opensearch-dashboards configured so I tried to add opensearch-dashboards  myself but authorization and authentication was disabled in this configuration so I was unable to login in the web admin interface of Opensearch.  In the end I decided to make my own docker-compose from scratch which led to the above errors.<br><strong>Relevant Logs or Screenshots</strong>:<br>
data prepper docker logs</p><pre><code class="lang-auto">2023-10-05 13:55:29 WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
2023-10-05 13:55:50 2023-10-05T11:55:50,660 [main] INFO  com.amazon.dataprepper.parser.config.DataPrepperAppConfiguration - Command line args: /usr/share/data-prepper/pipelines.yaml,/usr/share/data-prepper/data-prepper-config.yaml
2023-10-05 13:55:50 2023-10-05T11:55:50,695 [main] INFO  com.amazon.dataprepper.parser.config.DataPrepperArgs - Using /usr/share/data-prepper/pipelines.yaml configuration file
2023-10-05 13:56:32 2023-10-05T11:56:32,488 [main] WARN  com.amazon.dataprepper.parser.model.PipelineConfiguration - Prepper configurations are deprecated, processor configurations will be required in Data Prepper 2.0
2023-10-05 13:56:32 2023-10-05T11:56:32,559 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building pipeline [entry-pipeline] from provided configuration
2023-10-05 13:56:32 2023-10-05T11:56:32,570 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building [otel_trace_source] as source component for the pipeline [entry-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,168 [main] WARN  com.amazon.dataprepper.plugins.source.oteltrace.OTelTraceSource - Creating otel-trace-source without authentication. This is not secure.
2023-10-05 13:56:33 2023-10-05T11:56:33,170 [main] WARN  com.amazon.dataprepper.plugins.source.oteltrace.OTelTraceSource - In order to set up Http Basic authentication for the otel-trace-source, go here: https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/otel-trace-source#authentication-configurations
2023-10-05 13:56:33 2023-10-05T11:56:33,187 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building buffer for the pipeline [entry-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,491 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building processors for the pipeline [entry-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,500 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building sinks for the pipeline [entry-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,506 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building [pipeline] as sink component
2023-10-05 13:56:33 2023-10-05T11:56:33,528 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building pipeline [raw-pipeline] from provided configuration
2023-10-05 13:56:33 2023-10-05T11:56:33,531 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building [pipeline] as source component for the pipeline [raw-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,533 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building buffer for the pipeline [raw-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,564 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building processors for the pipeline [raw-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,573 [main] WARN  com.amazon.dataprepper.parser.PipelineParser - No plugin of type Processor found for plugin setting: otel_trace_raw_prepper, attempting to find comparable Prepper plugin.
2023-10-05 13:56:33 2023-10-05T11:56:33,745 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building sinks for the pipeline [raw-pipeline]
2023-10-05 13:56:33 2023-10-05T11:56:33,751 [main] INFO  com.amazon.dataprepper.parser.PipelineParser - Building [opensearch] as sink component
2023-10-05 13:56:34 2023-10-05T11:56:34,109 [main] INFO  com.amazon.dataprepper.plugins.sink.opensearch.OpenSearchSink - Initializing OpenSearch sink
2023-10-05 13:56:36 2023-10-05T11:56:36,857 [main] INFO  com.amazon.dataprepper.plugins.sink.opensearch.ConnectionConfiguration - Using the trust all strategy
2023-10-05 13:56:38 2023-10-05T11:56:38,087 [main] ERROR com.amazon.dataprepper.plugin.PluginCreator - Encountered exception while instantiating the plugin OpenSearchSink
2023-10-05 13:56:38 java.lang.reflect.InvocationTargetException: null
2023-10-05 13:56:38 at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]
2023-10-05 13:56:38 at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64) ~[?:?]
2023-10-05 13:56:38 at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]
2023-10-05 13:56:38 at java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[?:?]
2023-10-05 13:56:38 at java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[?:?]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugin.PluginCreator.newPluginInstance(PluginCreator.java:40) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugin.DefaultPluginFactory.loadPlugin(DefaultPluginFactory.java:66) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.parser.PipelineParser.buildSinkOrConnector(PipelineParser.java:180) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ~[?:?]
2023-10-05 13:56:38 at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) ~[?:?]
2023-10-05 13:56:38 at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ~[?:?]
2023-10-05 13:56:38 at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]
2023-10-05 13:56:38 at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]
2023-10-05 13:56:38 at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]
2023-10-05 13:56:38 at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]
2023-10-05 13:56:38 at com.amazon.dataprepper.parser.PipelineParser.buildPipelineFromConfiguration(PipelineParser.java:107) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.parser.PipelineParser.parseConfiguration(PipelineParser.java:72) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.DataPrepper.&lt;init&gt;(DataPrepper.java:57) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]
2023-10-05 13:56:38 at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64) ~[?:?]
2023-10-05 13:56:38 at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]
2023-10-05 13:56:38 at java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[?:?]
2023-10-05 13:56:38 at java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[?:?]
2023-10-05 13:56:38 at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:211) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:117) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:311) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:296) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:887) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:791) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:541) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:887) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:791) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:541) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1352) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1195) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1391) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1311) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:887) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:791) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:229) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1372) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1222) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:582) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:918) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.ContextManager.&lt;init&gt;(ContextManager.java:48) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.DataPrepperExecute.main(DataPrepperExecute.java:22) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 Caused by: java.lang.RuntimeException: opensearch: Name or service not known
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.OpenSearchSink.&lt;init&gt;(OpenSearchSink.java:92) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 ... 82 more
2023-10-05 13:56:38 Caused by: java.io.IOException: opensearch: Name or service not known
2023-10-05 13:56:38 at org.opensearch.client.RestClient.extractAndWrapCause(RestClient.java:912) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.RestClient.performRequest(RestClient.java:301) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.RestClient.performRequest(RestClient.java:289) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1762) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1728) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1696) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at org.opensearch.client.ClusterClient.getSettings(ClusterClient.java:119) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.index.IndexManager.checkISMEnabled(IndexManager.java:149) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.index.IndexManager.checkAndCreateIndexTemplate(IndexManager.java:165) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.index.IndexManager.setupIndex(IndexManager.java:160) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.OpenSearchSink.initialize(OpenSearchSink.java:105) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 at com.amazon.dataprepper.plugins.sink.opensearch.OpenSearchSink.&lt;init&gt;(OpenSearchSink.java:89) ~[data-prepper.jar:1.5.1]
2023-10-05 13:56:38 ... 82 more

</code></pre><p>opentelemetry docker logs:</p><pre><code class="lang-auto">2023-10-05 13:55:25 2023/10/05 11:55:25 proto: duplicate proto type registered: jaeger.api_v2.PostSpansRequest
2023-10-05 13:55:25 2023/10/05 11:55:25 proto: duplicate proto type registered: jaeger.api_v2.PostSpansResponse
2023-10-05 13:55:25 2023-10-05T11:55:25.683Zinfoservice/telemetry.go:115Setting up own telemetry...
2023-10-05 13:55:25 2023-10-05T11:55:25.685Zinfoservice/telemetry.go:156Serving Prometheus metrics{"address": ":8888", "level": "basic"}
2023-10-05 13:55:25 2023-10-05T11:55:25.692Zinfoservice/service.go:112Starting otelcol-contrib...{"Version": "0.59.0", "NumCPU": 4}
2023-10-05 13:55:25 2023-10-05T11:55:25.693Zinfoextensions/extensions.go:42Starting extensions...
2023-10-05 13:55:25 2023-10-05T11:55:25.696Zinfopipelines/pipelines.go:74Starting exporters...
2023-10-05 13:55:25 2023-10-05T11:55:25.697Zinfopipelines/pipelines.go:78Exporter is starting...{"kind": "exporter", "data_type": "traces", "name": "otlp/2"}
2023-10-05 13:55:25 2023-10-05T11:55:25.708Zinfopipelines/pipelines.go:82Exporter started.{"kind": "exporter", "data_type": "traces", "name": "otlp/2"}
2023-10-05 13:55:25 2023-10-05T11:55:25.711Zinfopipelines/pipelines.go:86Starting processors...
2023-10-05 13:55:25 2023-10-05T11:55:25.713Zinfopipelines/pipelines.go:90Processor is starting...{"kind": "processor", "name": "batch", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.713Zinfopipelines/pipelines.go:94Processor started.{"kind": "processor", "name": "batch", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.713Zinfopipelines/pipelines.go:98Starting receivers...
2023-10-05 13:55:25 2023-10-05T11:55:25.713Zinfopipelines/pipelines.go:102Receiver is starting...{"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.714Zinfootlpreceiver/otlp.go:70Starting GRPC server on endpoint 0.0.0.0:4317{"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.715Zinfootlpreceiver/otlp.go:88Starting HTTP server on endpoint 0.0.0.0:4318{"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.715Zinfopipelines/pipelines.go:106Receiver started.{"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-10-05 13:55:25 2023-10-05T11:55:25.715Zinfoservice/service.go:129Everything is ready. Begin running and processing data.
2023-10-05 13:55:25 2023-10-05T11:55:25.759Zwarnzapgrpc/zapgrpc.go:191[core] [Channel #1 SubChannel #2] grpc: addrConn.createTransport failed to connect to {
2023-10-05 13:55:25   "Addr": "data-prepper:21890",
2023-10-05 13:55:25   "ServerName": "data-prepper:21890",
2023-10-05 13:55:25   "Attributes": null,
2023-10-05 13:55:25   "BalancerAttributes": null,
2023-10-05 13:55:25   "Type": 0,
2023-10-05 13:55:25   "Metadata": null
2023-10-05 13:55:25 }. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.32.2:21890: connect: connection refused"{"grpc_log": true}
</code></pre><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/failing-to-send-traces-from-opentelemetry-dataprepper-to-opensearch/16159">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/failing-to-send-traces-from-opentelemetry-dataprepper-to-opensearch/16159</link>
            <pubDate>Thu, 05 Oct 2023 12:37:36 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16159</guid>
            <source url="https://forum.opensearch.org/t/failing-to-send-traces-from-opentelemetry-dataprepper-to-opensearch/16159.rss">Failing to send traces from Opentelemetry DataPrepper to Opensearch</source>
        </item>
        <item>
            <title>LTR Model and Model Serving Framework</title>
            <dc:creator>
                <![CDATA[anshuk]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Open Search 2.9</p><p><strong>Describe the issue</strong>:</p><p>We have been working LTR Plugin for search relevancy. In this context, as per the LTR needs,</p><p>The feature set needs to be created using api<br>
The logging feature has to be invoked using api<br>
Accordingly the judgment list needs to be created<br>
The model (e.g. xgboost) has be trained<br>
The model needs to be uploded.</p><p>Wanted to understand with model serving framework in place, can all the steps for ltr be done using the model serving framework and pipeline. If yes, can you refer to an example.</p><p>If not, do we have any plans for this down the line</p><p><strong>Configuration</strong>:</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/ltr-model-and-model-serving-framework/16156">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/ltr-model-and-model-serving-framework/16156</link>
            <pubDate>Thu, 05 Oct 2023 08:02:13 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16156</guid>
            <source url="https://forum.opensearch.org/t/ltr-model-and-model-serving-framework/16156.rss">LTR Model and Model Serving Framework</source>
        </item>
        <item>
            <title>Logstash input plugin s3snssqs issue</title>
            <dc:creator>
                <![CDATA[raj1209]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p>Hi Team,</p><p>I have been tetsing logstash <strong>s3snssqs</strong> plugin to fetch logs from sqs s3.</p><p>Earlier the stack was s3 ← sqs ← filebeat → logstash → opensearch</p><p>Now we are trying to eliminate filebeat and want to grab logs through the input plugin<br>
It worked fine in dev env ( test ) and now I am trying to implement the same in qa env but while configuring the s3snssqs plugin as input we are receiving a log count is 100’s ( filebeat service was stopped at this point )  where as  with filebeat config we are receiving as expected logs ( which is in min 1k logs per minute )</p><p>Anyone faced this type of issue with this plugin ( s3snssqs ) or is it something happening in the backend between sqs and the plugin trying to fetch the logs.</p><p>Here’s the sample snippet of the logstash config input section</p><pre><code class="lang-auto">  s3snssqs {
    region                     =&gt; "us-west-1"
    from_sns                   =&gt; false
    consumer_threads           =&gt; 2
    s3_default_options         =&gt; { "endpoint_discovery" =&gt; true }
    queue                      =&gt; "n11-us-va-03-alb-logs-message-queue"
    access_key_id              =&gt; "#########"
    secret_access_key          =&gt; "#############"
    type                       =&gt; "sqs-logs"
    tags                       =&gt; ["alb-logs"]
    sqs_skip_delete            =&gt; false
    codec                      =&gt; json
    s3_options_by_bucket       =&gt; [
      { bucket_name =&gt; "n11-us-va-03-alb-logs-message-queue"
        folders =&gt; [
          { key =&gt; ".*/alb.*"
            codec =&gt; "json_stream"
            type =&gt; "ALB"}
        ]
      }
    ]
  }
</code></pre><p>Can anyone please help me with this please, Thank you</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/logstash-input-plugin-s3snssqs-issue/16154">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/logstash-input-plugin-s3snssqs-issue/16154</link>
            <pubDate>Thu, 05 Oct 2023 06:58:15 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16154</guid>
            <source url="https://forum.opensearch.org/t/logstash-input-plugin-s3snssqs-issue/16154.rss">Logstash input plugin s3snssqs issue</source>
        </item>
        <item>
            <title>How to find conflicting mappings</title>
            <dc:creator>
                <![CDATA[stecino]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong>  2.9.0</p><p>When I go to my index pattern in dashboards, it says that there are 12 mapping conflicts, or sometimes that number may change. With earlier versions of Kibana typing conflict would display the fields. I tried same thing in dashboards but no dice. Is there a way to identify them? I have 1400 fields in my index</p><p><small>2 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/how-to-find-conflicting-mappings/16153">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/how-to-find-conflicting-mappings/16153</link>
            <pubDate>Wed, 04 Oct 2023 23:40:26 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16153</guid>
            <source url="https://forum.opensearch.org/t/how-to-find-conflicting-mappings/16153.rss">How to find conflicting mappings</source>
        </item>
        <item>
            <title>NEWBIE HELP! Clean install not working</title>
            <dc:creator>
                <![CDATA[McJava1967]]>
            </dc:creator>
            <category>Open Source Elasticsearch and Kibana</category>
            <description>
                <![CDATA[
            <p>Hi all.  Please help me get OpenSearch running!  <img src="https://emoji.discourse-cdn.com/google/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p><p>I installed latest using yum, and changed 9200 port to 9201, to allow nginx.  Otherwise, it’s basically out-of-the-box.</p><p>Both OpenSearch and Dashboards start.  But Dashboards can’t connect to OpenSearch.  I don’t know what other configuration is needed.</p><p>Any help would be greatly appreciated!</p><p>This is the Dashboard log:</p><pre><code class="lang-auto">Oct 04 19:15:52 myserver opensearch-dashboards[110119]: {"type":"log","@timestamp":"2023-10-04T19:15:52Z","tags":["warning","savedobjects-service"],"pid":110119,"message":"Unable to connect to OpenSearch. Error: Given the configuration, the ConnectionPool was not able to find a usable Connection for this request."}
Oct 04 19:37:28 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:28Z","tags":["info","plugins-service"],"pid":115045,"message":"Plugin \"dataSourceManagement\" has been disabled since the following direct or transitive dependencies are missing or disabled: [dataSource]"}
Oct 04 19:37:28 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:28Z","tags":["info","plugins-service"],"pid":115045,"message":"Plugin \"dataSource\" is disabled."}
Oct 04 19:37:28 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:28Z","tags":["info","plugins-service"],"pid":115045,"message":"Plugin \"visTypeXy\" is disabled."}
Oct 04 19:37:28 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:28Z","tags":["warning","config","deprecation"],"pid":115045,"message":"\"opensearch.requestHeadersWhitelist\" is deprecated and has been replaced by \"opensearch.requestHeadersAllowlist\""}
Oct 04 19:37:28 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:28Z","tags":["info","plugins-system"],"pid":115045,"message":"Setting up [51] plugins: [usageCollection,opensearchDashboardsUsageCollection,opensearchDashboardsLegacy,mapsLegacy,share,opensearchUiShared,legacyExport,embeddable,expressions,data,securityAnalyticsDashboards,home,apmOss,savedObjects,searchRelevanceDashboards,reportsDashboards,dashboard,mlCommonsDashboards,visualizations,visTypeVega,visTypeTimeline,visTypeTable,visTypeMarkdown,visBuilder,visAugmenter,anomalyDetectionDashboards,alertingDashboards,tileMap,regionMap,customImportMapDashboards,inputControlVis,ganttChartDashboards,visualize,queryWorkbenchDashboards,indexManagementDashboards,notificationsDashboards,management,indexPatternManagement,advancedSettings,console,dataExplorer,charts,visTypeVislib,visTypeTimeseries,visTypeTagcloud,visTypeMetric,observabilityDashboards,discover,savedObjectsManagement,securityDashboards,bfetch]"}
Oct 04 19:37:29 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:29Z","tags":["info","savedobjects-service"],"pid":115045,"message":"Waiting until all OpenSearch nodes are compatible with OpenSearch Dashboards before starting saved objects migrations..."}
Oct 04 19:37:29 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:29Z","tags":["error","opensearch","data"],"pid":115045,"message":"[ConnectionError]: write EPROTO 140466277648256:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:\n"}
Oct 04 19:37:29 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:29Z","tags":["error","savedobjects-service"],"pid":115045,"message":"Unable to retrieve version information from OpenSearch nodes."}
Oct 04 19:37:31 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:31Z","tags":["error","opensearch","data"],"pid":115045,"message":"[ConnectionError]: write EPROTO 140466277648256:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:\n"}
Oct 04 19:37:34 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:34Z","tags":["error","opensearch","data"],"pid":115045,"message":"[ConnectionError]: write EPROTO 140466277648256:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:\n"}
Oct 04 19:37:36 myserver opensearch-dashboards[115045]: {"type":"log","@timestamp":"2023-10-04T19:37:36Z","tags":["error","opensearch","data"],"pid":115045,"message":"[ConnectionError]: write EPROTO 140466277648256:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:\n"}

</code></pre><p>This is the opensearch.yml:</p><pre><code class="lang-auto">cluster.name: skynet-elk
path.data: /ds1/opensearch
path.logs: /var/log/opensearch
http.port: 9201

plugins.security.ssl.transport.pemcert_filepath: esnode.pem
plugins.security.ssl.transport.pemkey_filepath: esnode-key.pem
plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem
plugins.security.ssl.transport.enforce_hostname_verification: false
plugins.security.ssl.http.enabled: true
plugins.security.ssl.http.pemcert_filepath: esnode.pem
plugins.security.ssl.http.pemkey_filepath: esnode-key.pem
plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem
plugins.security.allow_unsafe_democertificates: true
plugins.security.allow_default_init_securityindex: true
plugins.security.authcz.admin_dn:
  - CN=kirk,OU=client,O=client,L=test, C=de

plugins.security.audit.type: internal_opensearch
plugins.security.enable_snapshot_restore_privilege: true
plugins.security.check_snapshot_restore_write_privileges: true
plugins.security.restapi.roles_enabled: ["all_access", "security_rest_api_access"]
plugins.security.system_indices.enabled: true
plugins.security.system_indices.indices: [".plugins-ml-config", ".plugins-ml-connector", ".plugins-ml-model-group", ".plugins-ml-model", ".plugins-ml-task", ".plugins-ml-conversation-meta", ".plugins-ml-conversation-interactions", ".opendistro-alerting-config", ".opendistro-alerting-alert*", ".opendistro-anomaly-results*", ".opendistro-anomaly-detector*", ".opendistro-anomaly-checkpoints", ".opendistro-anomaly-detection-state", ".opendistro-reports-*", ".opensearch-notifications-*", ".opensearch-notebooks", ".opensearch-observability", ".ql-datasources", ".opendistro-asynchronous-search-response*", ".replication-metadata-store", ".opensearch-knn-models", ".geospatial-ip2geo-data*"]
node.max_local_storage_nodes: 3
</code></pre><p>And this is the opensearch-dashboards.yml:</p><pre><code class="lang-auto">---
opensearch.hosts: [https://localhost:9200]
opensearch.ssl.verificationMode: none
opensearch.username: kibanaserver
opensearch.password: kibanaserver
opensearch.requestHeadersWhitelist: [authorization, securitytenant]
opensearch_security.multitenancy.enabled: true
opensearch_security.multitenancy.tenants.preferred: [Private, Global]
opensearch_security.readonly_mode.roles: [kibana_read_only]
opensearch_security.cookie.secure: false
</code></pre><p>THANK YOU!!!  <img src="https://emoji.discourse-cdn.com/google/slight_smile.png?v=12" title=":slight_smile:" class="emoji" alt=":slight_smile:" loading="lazy" width="20" height="20"></p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/newbie-help-clean-install-not-working/16152">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/newbie-help-clean-install-not-working/16152</link>
            <pubDate>Wed, 04 Oct 2023 20:28:24 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16152</guid>
            <source url="https://forum.opensearch.org/t/newbie-help-clean-install-not-working/16152.rss">NEWBIE HELP! Clean install not working</source>
        </item>
        <item>
            <title>Getting rid of unwanted commas in long formatted data</title>
            <dc:creator>
                <![CDATA[stecino]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> 2.9.0</p><p>At the moment I am using dynamic template for my indices. In reality, the only thing I specify in there is the primary/replica shard allocation count. Everything else is dynamically generated. At the moment fields that are written as long have comma in them. Is there a way to get rid of it? Do i need to convert them to integer or double? My mapping may change over time as I write different events to index, but would it be possible to explicitly define the field types in the mapping template for the fields that I know, and still allow for other fields to be genrated dynamically?</p><p>For example for this snippet in the mapping</p><pre><code class="lang-auto">                    "inode" : {
                      "type" : "long"
                    },
</code></pre><p>Can I define mapping for this node as integer or double?</p><p><small>2 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/getting-rid-of-unwanted-commas-in-long-formatted-data/16149">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/getting-rid-of-unwanted-commas-in-long-formatted-data/16149</link>
            <pubDate>Wed, 04 Oct 2023 18:40:28 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16149</guid>
            <source url="https://forum.opensearch.org/t/getting-rid-of-unwanted-commas-in-long-formatted-data/16149.rss">Getting rid of unwanted commas in long formatted data</source>
        </item>
        <item>
            <title>Snapshot management policies not working</title>
            <dc:creator>
                <![CDATA[msmsx]]>
            </dc:creator>
            <category>Index Management</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
OpenSearch v 2.6.0</p><p><strong>Describe the issue</strong>:<br>
Hi to all<br>
I have 3-node wazuh indexer cluster based on OS 2.6.0 and want to set snapshot management policy to take snapshots every 5 hours to external nfs share for example<br>
snapshot management not working and i see next repeating log entry in /var/log/wazuh-indexer/wazuh-cluster.log</p><blockquote><p>[WARN ][o.o.i.s.SMRunner         ] [node-1] Cannot acquire lock for snapshot management job test_policy</p></blockquote><p>On screenshot policy has a different name but it doesn’t matter<br>
I’ll add that if i launch snapshot manually it will be created so no issues with NFS access for read/write<br>
Also had tested snapshots management policy with another wazuh dedicated for tests with only 1 wazuh-indexer based on OS 2.6.0 and it work with same NFS storage</p><p><strong>Configuration</strong>:<br>
Steps to configure snapshot management policy:</p><ol><li>Add repo to opensearch.yml on each cluster node</li><li>Create repo in UI</li><li>Create snapshot policy in UI using cron scheduler (policy screenshot in attach)</li></ol><p><strong>Relevant Logs or Screenshots</strong>:<br>
[WARN ][o.o.i.s.SMRunner         ] [node-1] Cannot acquire lock for snapshot management job test_policy</p><p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/d/dc3ec4f76a9291ea301233adad257f4d8693ed77.png" data-download-href="/uploads/short-url/vqnwDatc8RZ1nYrVCjbo0MLEmXB.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/d/dc3ec4f76a9291ea301233adad257f4d8693ed77_2_690x356.png" alt="image" data-base62-sha1="vqnwDatc8RZ1nYrVCjbo0MLEmXB" width="690" height="356" srcset="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/d/dc3ec4f76a9291ea301233adad257f4d8693ed77_2_690x356.png, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/d/dc3ec4f76a9291ea301233adad257f4d8693ed77_2_1035x534.png 1.5x, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/d/dc3ec4f76a9291ea301233adad257f4d8693ed77_2_1380x712.png 2x" data-dominant-color="FBFCFC"></a></div><p></p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/snapshot-management-policies-not-working/16147">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/snapshot-management-policies-not-working/16147</link>
            <pubDate>Wed, 04 Oct 2023 17:15:03 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16147</guid>
            <source url="https://forum.opensearch.org/t/snapshot-management-policies-not-working/16147.rss">Snapshot management policies not working</source>
        </item>
        <item>
            <title>Opensearch: ARCInvalidSignatureException (403)</title>
            <dc:creator>
                <![CDATA[Dijenliu]]>
            </dc:creator>
            <category>OpenSearch Dashboards</category>
            <description>
                <![CDATA[
            <p>Opensearch version : 2.9</p><p>I recently upgraded OS version to 2.9 and noticed this error while trying to create dashboards from my visualization.<br>
“aws.auth.client.error.ARCInvalidSignatureException: Invalid signature, does not match”<br>
Every other feature after the migration works just fine.<br>
We use AWS cognito and I have revisited all the roles. I had done the same set up(Cognito included)  with OS version 2.7 and it worked perfectly fine.</p><p></p><div class="lightbox-wrapper"><a class="lightbox" href="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/5/50507cbaea3a0f05b293a3b252885fd0b049c4b0.png" data-download-href="/uploads/short-url/bsuDIWD7IHx5vdZarBulftBoNEc.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/5/50507cbaea3a0f05b293a3b252885fd0b049c4b0_2_690x388.png" alt="image" data-base62-sha1="bsuDIWD7IHx5vdZarBulftBoNEc" width="690" height="388" srcset="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/optimized/2X/5/50507cbaea3a0f05b293a3b252885fd0b049c4b0_2_690x388.png, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/5/50507cbaea3a0f05b293a3b252885fd0b049c4b0.png 1.5x, https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/5/50507cbaea3a0f05b293a3b252885fd0b049c4b0.png 2x" data-dominant-color="F4F5F5"></a></div><p></p><p>In addition to this, my Opensearch console just stops working after this error and requires me to re log-in.</p><p>Even trying to load Opensearch provided sample dashboard([Flights] Global Flight Dashboard) is throwing an error.</p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/opensearch-arcinvalidsignatureexception-403/16143">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/opensearch-arcinvalidsignatureexception-403/16143</link>
            <pubDate>Wed, 04 Oct 2023 16:23:11 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16143</guid>
            <source url="https://forum.opensearch.org/t/opensearch-arcinvalidsignatureexception-403/16143.rss">Opensearch: ARCInvalidSignatureException (403)</source>
        </item>
        <item>
            <title>Automated Index Rollup</title>
            <dc:creator>
                <![CDATA[Jaro]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Docker based v2.8</p><p><strong>Describe the issue</strong>:<br>
I use OpenSearch along with Logstash and Filebeat to ingest and analyze logs and extract metrics.</p><p>The index names are created in the Logstash, here some examples:</p><ul><li>component1_r_m-2023.39, where r_m states for monthly retention and 39 is the week number which means that the index is created weekly; or</li><li>component2_r_w-2023.10.04, where r_w states for weekly retention and the index is created daily</li></ul><p>I have three ISM policies which delete the indices:</p><ul><li>monthly for the index pattern <em>r_m</em></li><li>weekly for the index pattern <em>r_w</em></li><li>yearly for the index pattern <em>r_y</em></li></ul><p>What I want to achieve is to add via API a rollup job which would take the weekly index, aggregate data into a new index which would be deleted after a year. Ideally I would like to use the current retention policies.</p><p>For instance all indices component1_r_m-2023.30 … - … component1_r_m-2023.30 would be aggregated to rollup-component1_r_y-2023.3 and component1_r_m-2023.40 … - … component1_r_m-2023.49 would be aggregated to rollup-component1_r_y-2023.4, and so on.</p><p>In a nutshell, I want to achieve what index rollup is intended to do, aggregated data and keep if for longer in less number of indices or shards.</p><p><strong>Configuration</strong>:<br>
First I tried the Rollup Jobs. Here though there are 2 issues.</p><ol><li>It’s not possible to create the dynamic target_index from an index pattern</li><li>But even though I prepared several rollup jobs which should be installed now the ones which are intended to be used in the future are failing and not getting re-eanbled when the index matching the source_index pattern has been created.</li></ol><p>For the reference there were 2 rollup jobs created:</p><p>Job1<br>
“source_index”: “component1_r_m-2023.3*”,<br>
“target_index”: “rollup-component1_r_y-2023.3”</p><p>Job2<br>
“source_index”: “component1_r_m-2023.4*”,<br>
“target_index”: “rollup-component1_r_y-2023.4”</p><p>The Job1 was enabled but Job 2 failed due to unavailability of the source_index and disabled.<br>
So, this method will not work.</p><p>So I’m trying now with the ISM rollover with rollup action.</p><p>I’ve added the alias and ism rollover alias to the index template.</p><p>PUT _index_template/component1<br>
{<br>
“index_patterns”: [<br>
“component1*”<br>
],<br>
“priority”: 5,<br>
“template”: {<br>
“aliases”: {<br>
“xxyyzz”: {<br>
“is_write_index”: true<br>
}<br>
},<br>
“settings”: {<br>
“number_of_shards”: “1”,<br>
“refresh_interval”: “30s”,<br>
“plugins.index_state_management.rollover_alias”: “xxyyzz”<br>
},<br>
“mappings”: {<br>
“dynamic”: false,</p><p>When checking the details of the index I see that the alias, write index and rollover_alias are there<br>
{<br>
“component1_r_w-2023.39-0001”: {<br>
“aliases”: {<br>
“xxyyzz”: {<br>
“is_write_index”: true<br>
}<br>
},<br>
…<br>
“settings”: {<br>
“index”: {<br>
“refresh_interval”: “30s”,<br>
“number_of_shards”: “1”,<br>
“plugins”: {<br>
“index_state_management”: {<br>
“rollover_alias”: “xxyyzz”<br>
}<br>
},</p><p>The ISM policy is a per the prescription in the OpenSearch docu:</p><p>PUT _plugins/_ism/policies/rollover_policy_component1<br>
{<br>
“policy”: {<br>
“description”: “Example rollover policy component1.”,<br>
“default_state”: “rollover”,<br>
“states”: [<br>
{<br>
“name”: “rollover”,<br>
“actions”: [<br>
{<br>
“rollover”: {<br>
“min_doc_count”: 1<br>
}<br>
}<br>
],<br>
“transitions”: [<br>
{<br>
“state_name”: “rp”<br>
}<br>
]<br>
},<br>
{<br>
“name”: “rp”,<br>
“actions”: [<br>
{<br>
“rollup”: {<br>
“ism_rollup”: {<br>
“target_index”: “rollup_ndx-{{ctx.source_index}}”,<br>
“description”: “Example rollup job”,<br>
“page_size”: 5,<br>
“dimensions”: [<br>
{<br>
“date_histogram”: {<br>
“source_field”: “ts”,<br>
“fixed_interval”: “5m”,<br>
“timezone”: “UTC”<br>
…<br>
],<br>
“metrics”: <span class="chcklst-box fa fa-square-o fa-fw"></span><br>
}<br>
}<br>
}<br>
],<br>
“transitions”: <span class="chcklst-box fa fa-square-o fa-fw"></span><br>
}<br>
],<br>
“ism_template”: {<br>
“index_patterns”: [“component1*”],<br>
“priority”: 80<br>
}<br>
}<br>
}</p><p>This is the latest error I’m facing. The rollover alias should be pointing to multiple indices therefore I’m puzzled how to move on and if I will be able to fulfill my requirements.<br>
{<br>
“cause”: “Rollover alias [xxyyzz] can point to multiple indices, found duplicated alias [[xxyyzz]] in index template [component1]”,<br>
“message”: “Failed to rollover index [index=component1_r_w-2023.39-0001]”<br>
}</p><p>It would be much easier and intuitive if the target_index could be dynamically created but without dependency to the source_index. This could have been done using solution similar to the one in Logstash.</p><p>If I could create a rollup job<br>
“source_index”: “component1_r_m-*”,<br>
“target_index”: “rollup-component1_r_y-%{+YYYY.MM}”</p><p>then data in source indices would be aggregated to the monthly indices which would be dropped after a year.</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>2 posts - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/automated-index-rollup/16137">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/automated-index-rollup/16137</link>
            <pubDate>Wed, 04 Oct 2023 12:51:21 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16137</guid>
            <source url="https://forum.opensearch.org/t/automated-index-rollup/16137.rss">Automated Index Rollup</source>
        </item>
        <item>
            <title>Java Client not working for OpenSearch document insert</title>
            <dc:creator>
                <![CDATA[lakshminarasimhan24]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser): Opensearch</p><aside class="onebox allowlistedgeneric" data-onebox-src="https://opensearch.org/docs/1.2/clients/java/"><header class="source"><img src="https://global.discourse-cdn.com/business5/uploads/mauve_hedgehog/original/2X/4/428b125d9a7822dd28ea5ba5703b82658cec48b6.png" class="site-icon" data-dominant-color="004F91" width="128" height="128"><a href="https://opensearch.org/docs/1.2/clients/java/" target="_blank" rel="noopener nofollow ugc" title="05:18PM - 18 September 2023">OpenSearch documentation – 18 Sep 23</a></header><article class="onebox-body"><h3><a href="https://opensearch.org/docs/1.2/clients/java/" target="_blank" rel="noopener nofollow ugc">Java client</a></h3><p>Java client</p></article><div class="onebox-metadata">


  </div><div style="clear: both"></div></aside><pre><code class="lang-auto">OpenSearchClientExample.java when run with mentioned dependencies throws java.lang.invoke.LambdaConversionException


</code></pre><p><strong>Configuration</strong>:<br>
org.opensearch.client<br>
opensearch-rest-client<br>
1.2.4<br><br><br>
org.opensearch.client<br>
opensearch-java<br>
0.1.0<br></p><p><strong>Relevant Logs or Screenshots</strong>:</p><p>Exception in thread “main” java.lang.BootstrapMethodError: bootstrap method initialization exception<br>
at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:194)<br>
at java.base/java.lang.invoke.CallSite.makeSite(CallSite.java:307)<br>
at java.base/java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:258)<br>
at java.base/java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:248)<br>
at org.opensearch.client.RestClient.convertResponse(RestClient.java:330)<br>
at org.opensearch.client.RestClient.performRequest(RestClient.java:314)<br>
at org.opensearch.client.RestClient.performRequest(RestClient.java:289)<br>
at org.opensearch.client.base.RestClientTransport.performRequest(RestClientTransport.java:77)<br>
at org.opensearch.client.opensearch.OpenSearchClient.index(OpenSearchClient.java:641)<br>
at com.dmat.lambda.OpenSearchClientExample.main(OpenSearchClientExample.java:60)<br>
Caused by: java.lang.invoke.LambdaConversionException: Invalid receiver type interface org.apache.http.Header; not a subtype of implementation type interface org.apache.http.NameValuePair<br>
at java.base/java.lang.invoke.AbstractValidatingLambdaMetafactory.validateMetafactoryArgs(AbstractValidatingLambdaMetafactory.java:254)<br>
at java.base/java.lang.invoke.LambdaMetafactory.metafactory(LambdaMetafactory.java:328)<br>
at java.base/java.lang.invoke.BootstrapMethodInvoker.invoke(BootstrapMethodInvoker.java:127)<br>
… 9 more</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/java-client-not-working-for-opensearch-document-insert/16136">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/java-client-not-working-for-opensearch-document-insert/16136</link>
            <pubDate>Wed, 04 Oct 2023 12:27:08 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16136</guid>
            <source url="https://forum.opensearch.org/t/java-client-not-working-for-opensearch-document-insert/16136.rss">Java Client not working for OpenSearch document insert</source>
        </item>
        <item>
            <title>Using Filter and Query context together</title>
            <dc:creator>
                <![CDATA[yogesh.vaze]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):</p><p>Latest</p><p><strong>Describe the issue</strong>:</p><p>I am trying to search a set of words and also filter the results using filter and terms options, however, the query does not return anything. If I remove the filter and terms options, then it returns results.<br>
Need help in understanding why this happens.<br>
Here is the query I am using -<br>
search_query = {<br>
“query”: {<br>
“bool”: {<br>
“must”: {<br>
“simple_query_string”: {<br>
“query”: “Heart”,<br>
“fields”: [‘section’, ‘subsection’, ‘question’, ‘answer’],<br>
“default_operator”:“OR”</p><pre><code>                    }
                },
                "filter": [
                    {
                        "term": {"section": "Medicine"}
                    }

                 ]
            }
        }
    }
</code></pre><p><strong>Configuration</strong>:</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/using-filter-and-query-context-together/16133">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/using-filter-and-query-context-together/16133</link>
            <pubDate>Wed, 04 Oct 2023 11:32:32 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16133</guid>
            <source url="https://forum.opensearch.org/t/using-filter-and-query-context-together/16133.rss">Using Filter and Query context together</source>
        </item>
        <item>
            <title>OpenSearch alerting plugin</title>
            <dc:creator>
                <![CDATA[darshita]]>
            </dc:creator>
            <category>Alerting</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
Open Search 2.9.0<br>
Open Search Dashboard 2.9.0</p><p><strong>Describe the issue</strong>:<br>
I have an opensearch index which will keep indexing data for below fields<br>
requestorName, accountName, endpoint and health at every 15 mins.<br>
Example 1) RSSRequestor, account1, http:///…1.rss, UP.<br>
2) RSSRequestor, account2, http:///…2.rss, DOWN,<br>
3) DataRequestor, account1, http:///…any.com, DOWN.<br>
I want to generate alerts that can send individual email notification when health of requestor is DOWN.<br>
For the above example it should send 2 emails. 1) RssRequestor, account2 having endpoint http:///…2.rss is DOWN<br>
2)DataRequestor, account1 having endpoint http:///…any.com is DOWN.<br>
One requestor can have several accounts and each account can have multiple endpoints. I want to generate individual email for each requestor for all endpoints<br>
that are down. Also want to send alert if the endpoint is back UP. Can you provide full example of how to use open search alert for this scenario.<br>
I tried using per query monitor but it sends only single alert.<br><strong>Questions</strong><br>
Which monitor should I use? Can you guide on how to send multiple email notifications for each requesters that is down?<br>
Also want to send an email after the requester is back UP.</p><p>Tried with the below query monitor</p><pre><code class="lang-auto">  "query": {
        "bool": {
            "must": [
                {
                    "term": {
                        "health.keyword": {
                            "value": "DOWN",
                            "boost": 1
                        }
                    }
                }
            ],
            "adjust_pure_negative": true,
            "boost": 1
        }
    }
</code></pre><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/opensearch-alerting-plugin/16131">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/opensearch-alerting-plugin/16131</link>
            <pubDate>Wed, 04 Oct 2023 10:34:10 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16131</guid>
            <source url="https://forum.opensearch.org/t/opensearch-alerting-plugin/16131.rss">OpenSearch alerting plugin</source>
        </item>
        <item>
            <title>Query on opensearch security authetication/authorization</title>
            <dc:creator>
                <![CDATA[chaitra]]>
            </dc:creator>
            <category>Security</category>
            <description>
                <![CDATA[
            <p>Hi,<br>
I am using Opensearch, opensearch dashboards 2.9.0 version. Below is my query.<br>
Is it possible to completely disable authentication and authorization from security plugin and still access opensearch cluster?  (I know there is something called <code>anonymous_authentication</code>, but couldn’t find how configure roles/users for the same?)<br>
Apart from this are there any possibility to disable authentication and authorization?<br>
If yes please let me know about it.</p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/query-on-opensearch-security-authetication-authorization/16130">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/query-on-opensearch-security-authetication-authorization/16130</link>
            <pubDate>Wed, 04 Oct 2023 10:33:05 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16130</guid>
            <source url="https://forum.opensearch.org/t/query-on-opensearch-security-authetication-authorization/16130.rss">Query on opensearch security authetication/authorization</source>
        </item>
        <item>
            <title>Dashboard Refresh Desaturation</title>
            <dc:creator>
                <![CDATA[donnellyp]]>
            </dc:creator>
            <category>OpenSearch Dashboards</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> 2.10</p><p>Is there a way to disable the color desaturation when auto refreshing a dashboard?</p><p>The feature looks to have been removed in kibana. <a href="https://github.com/elastic/kibana/pull/97473/files/5e553ab20393ba2e10246ed43ec754a4b2f9f8b1" rel="noopener nofollow ugc">Kibana Change</a></p><p><small>2 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/dashboard-refresh-desaturation/16125">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/dashboard-refresh-desaturation/16125</link>
            <pubDate>Tue, 03 Oct 2023 20:21:08 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16125</guid>
            <source url="https://forum.opensearch.org/t/dashboard-refresh-desaturation/16125.rss">Dashboard Refresh Desaturation</source>
        </item>
        <item>
            <title>Opensearch cluster RED and not able to delete the RED old .opendistro indices</title>
            <dc:creator>
                <![CDATA[curiousmind]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
opensearch version 2.9.0<br>
opensearch_dashboards version 2.9.0</p><p><strong>Describe the issue</strong>:<br>
The issue here is we were upgrading the cluster and unforunatley 75% of the nodes went down.<br>
Now we managed with the remaining upgrade, but after upgrade the cluster is RED due to two indices not found (.opendistro-anomaly-detectors AND .opendistro-reports-instances).<br>
Now I tried to delete those indices with admin credentials, but getting the below response</p><pre><code class="lang-auto">{
  "error": {
    "root_cause": [
      {
        "type": "security_exception",
        "reason": "no permissions for [] and User [name=elastic, backend_roles=[], requestedTenant=null]"
      }
    ],
    "type": "security_exception",
    "reason": "no permissions for [] and User [name=elastic, backend_roles=[], requestedTenant=null]"
  },
  "status": 403
}
</code></pre><p>The user elastic is the admin user and I have ran out of ideas to delete this index.<br>
Tried via command line and via UI.</p><p>Can anyone help?</p><p><small>2 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/opensearch-cluster-red-and-not-able-to-delete-the-red-old-opendistro-indices/16123">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/opensearch-cluster-red-and-not-able-to-delete-the-red-old-opendistro-indices/16123</link>
            <pubDate>Tue, 03 Oct 2023 18:45:26 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16123</guid>
            <source url="https://forum.opensearch.org/t/opensearch-cluster-red-and-not-able-to-delete-the-red-old-opendistro-indices/16123.rss">Opensearch cluster RED and not able to delete the RED old .opendistro indices</source>
        </item>
        <item>
            <title>Rest client java code to fetch all indices</title>
            <dc:creator>
                <![CDATA[hello]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):</p><p><strong>Describe the issue</strong>:<br>
Please give a working java code that can fetch all indices of a cluster using java rest client version 2.1</p><p><strong>Configuration</strong>:</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>2 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/rest-client-java-code-to-fetch-all-indices/16122">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/rest-client-java-code-to-fetch-all-indices/16122</link>
            <pubDate>Tue, 03 Oct 2023 17:04:11 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16122</guid>
            <source url="https://forum.opensearch.org/t/rest-client-java-code-to-fetch-all-indices/16122.rss">Rest client java code to fetch all indices</source>
        </item>
        <item>
            <title>Update underlying libs</title>
            <dc:creator>
                <![CDATA[annvzel]]>
            </dc:creator>
            <category>SQL</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):</p><p>2.0.8,2.10.0</p><p><strong>Describe the issue</strong>:<br>
Hi,<br>
the Druid lib version included (1.0.15) is pretty old.<br>
Versions of alibaba-druid 1.0.0 - 1.1.19 are reported as vulnerable <a href="https://www.compass-security.com/fileadmin/Datein/Research/Advisories/CSNC-2019-022_alibaba-druid.txt" rel="noopener nofollow ugc">https://www.compass-security.com/fileadmin/Datein/Research/Advisories/CSNC-2019-022_alibaba-druid.txt</a></p><p>Is there a plan to update it?</p><p><strong>Configuration</strong>:</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/update-underlying-libs/16118">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/update-underlying-libs/16118</link>
            <pubDate>Tue, 03 Oct 2023 10:02:02 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16118</guid>
            <source url="https://forum.opensearch.org/t/update-underlying-libs/16118.rss">Update underlying libs</source>
        </item>
        <item>
            <title>Guidelines for new developer contribution</title>
            <dc:creator>
                <![CDATA[Abhilash]]>
            </dc:creator>
            <category>OpenSearch Dashboards</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):</p><p><strong>Describe the issue</strong>:<br>
For any new developer, it’s difficult to understand the guidelines for contribution.</p><p>=&gt; I don’t see any guidelines about the development branch to start.</p><p>=&gt; Every time <em>yarn start:docker</em> takes almost more than 20-25mins to get up. So is there any<br>
specific system configuration for this project to run?</p><p><strong>Configuration</strong>:</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>1 post - 1 participant</small></p><p><a href="https://forum.opensearch.org/t/guidelines-for-new-developer-contribution/16116">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/guidelines-for-new-developer-contribution/16116</link>
            <pubDate>Tue, 03 Oct 2023 07:21:35 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16116</guid>
            <source url="https://forum.opensearch.org/t/guidelines-for-new-developer-contribution/16116.rss">Guidelines for new developer contribution</source>
        </item>
        <item>
            <title>Why is `properties.` required in search terms</title>
            <dc:creator>
                <![CDATA[shacker]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
AWS OpenSearch (not sure where to find the version)</p><p><strong>Describe the issue</strong>:<br>
Newbie question, sorry.</p><p>All documentation I’ve seen for posting a document involves nesting the terms in a <code>properties</code> object:</p><pre><code class="lang-auto">POST locations-b/_doc/
{
  "properties": {
    "zipcode" : "02134"
  }
}
</code></pre><p>The document indexes successfully, and a <code>match_all</code> query shows the doc. But if I go to search for it, this turns up 0 hits:</p><pre><code class="lang-auto">GET locations-b/_search
{
  "query": {
    "match": {"zipcode": "02134"}
  }
}
</code></pre><p>In order to get hits, I need to specify the properties “container” in the query. This does work:</p><pre><code class="lang-auto">GET locations-b/_search
{
  "query": {
    "match": {"properties.zipcode": "02134"}
  }
}
</code></pre><p>But none of the search examples in documentation show that <code>properties.</code> needs to be specified in the query.</p><p>Am I doing something wrong? Or is this a quirk of the AWS implementation? Is there way to set up a mask or something so that it’s always implied? Thanks for any suggestions.</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/why-is-properties-required-in-search-terms/16113">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/why-is-properties-required-in-search-terms/16113</link>
            <pubDate>Tue, 03 Oct 2023 01:06:35 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16113</guid>
            <source url="https://forum.opensearch.org/t/why-is-properties-required-in-search-terms/16113.rss">Why is `properties.` required in search terms</source>
        </item>
        <item>
            <title>Open Search cluster is running high cpu and response time is also high</title>
            <dc:creator>
                <![CDATA[imdbtoimdb]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p><strong>Versions</strong> (relevant - OpenSearch/Dashboard/Server OS/Browser):<br>
version" : {<br>
“distribution” : “opensearch”,<br>
“number” : “1.2.2”,<br>
“build_type” : “tar”,<br>
“build_hash” : “123d41ce4fad54529acd7a290efed848e707b624”,<br>
“build_date” : “2021-12-15T18:03:07.761961Z”,<br>
“build_snapshot” : false,<br>
“lucene_version” : “8.10.1”,<br>
“minimum_wire_compatibility_version” : “6.8.0”,<br>
“minimum_index_compatibility_version” : “6.0.0-beta1”<br>
},</p><p><strong>Describe the issue</strong>:<br>
Hello All,<br>
We recently migrated our search cluster from ES (  “version” : {<br>
“number” : “6.8.16”,<br>
“build_flavor” : “default”,<br>
“build_type” : “deb”,<br>
“build_hash” : “1f62092”,<br>
“build_date” : “2021-05-21T19:27:57.985321Z”,<br>
“build_snapshot” : false,<br>
“lucene_version” : “7.7.3”,<br>
“minimum_wire_compatibility_version” : “5.6.0”,<br>
“minimum_index_compatibility_version” : “5.0.0”<br>
},</p><p>to open search . Wrt the H/W both are same . But we are seeing high cpu spike in open search  cluster .  not sure what are all the next step . Any help would be appreciated</p><p><strong>Configuration</strong>:<br>
No of shards : 24 on both OS and ES ( replica count is 1 per primary shard )  .</p><p><strong>Relevant Logs or Screenshots</strong>:</p><p><small>4 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/open-search-cluster-is-running-high-cpu-and-response-time-is-also-high/16111">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/open-search-cluster-is-running-high-cpu-and-response-time-is-also-high/16111</link>
            <pubDate>Mon, 02 Oct 2023 23:35:34 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16111</guid>
            <source url="https://forum.opensearch.org/t/open-search-cluster-is-running-high-cpu-and-response-time-is-also-high/16111.rss">Open Search cluster is running high cpu and response time is also high</source>
        </item>
        <item>
            <title>Error creating State Management Policies</title>
            <dc:creator>
                <![CDATA[jsamuel12]]>
            </dc:creator>
            <category>OpenSearch</category>
            <description>
                <![CDATA[
            <p>I created the following policy that will transition from hot to cold for all indices each day</p><pre><code class="lang-auto">{
    "id": "hot_to_cold_policy",
    "seqNo": 27868,
    "primaryTerm": 11,
    "policy": {
        "policy_id": "hot_to_cold_policy",
        "description": "Hot to cold transition, snapshot, and delete local index",
        "last_updated_time": 1696262321299,
        "schema_version": 18,
        "error_notification": null,
        "default_state": "hot",
        "states": [
            {
                "name": "hot",
                "actions": [],
                "transitions": [
                    {
                        "state_name": "cold",
                        "conditions": {
                            "min_index_age": "1d"
                        }
                    }
                ]
            },
            {
                "name": "cold",
                "actions": [
                    {
                        "retry": {
                            "count": 3,
                            "backoff": "exponential",
                            "delay": "1m"
                        },
                        "snapshot": {
                            "repository": "/opensearch_coldstorage/",
                            "snapshot": "{{ctx.index}}_snapshot"
                        }
                    },
                    {
                        "retry": {
                            "count": 3,
                            "backoff": "exponential",
                            "delay": "1m"
                        },
                        "delete": {}
                    }
                ],
                "transitions": []
            }
        ],
        "ism_template": [
            {
                "index_patterns": [
                    "*"
                ],
                "priority": 100,
                "last_updated_time": 1696262258189
            }
        ]
    }
}
</code></pre><p>My repository is a mounted s3 bucket which has full r/w access for the opensearch user BUT I’m receiving the following error</p><pre><code class="lang-auto">{
    "cause": "[/opensearch_coldstorage] missing",
    "message": "Failed to create snapshot [index=xxxx-2023.09.28]"
}
</code></pre><p>I can manually issue the <code>PUT /_snapshot/ColdStorage/1</code> command without any issues. Not sure why automatic task is failing.</p><p><small>3 posts - 2 participants</small></p><p><a href="https://forum.opensearch.org/t/error-creating-state-management-policies/16109">Read full topic</a></p>
          ]]>
            </description>
            <link>https://forum.opensearch.org/t/error-creating-state-management-policies/16109</link>
            <pubDate>Mon, 02 Oct 2023 20:38:26 +0000</pubDate>
            <discourse:topicPinned>No</discourse:topicPinned>
            <discourse:topicClosed>No</discourse:topicClosed>
            <discourse:topicArchived>No</discourse:topicArchived>
            <guid isPermaLink="false">forum.opensearch.org-topic-16109</guid>
            <source url="https://forum.opensearch.org/t/error-creating-state-management-policies/16109.rss">Error creating State Management Policies</source>
        </item>
    </channel>
</rss>